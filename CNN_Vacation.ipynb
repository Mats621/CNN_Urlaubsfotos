{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dee9acb",
   "metadata": {},
   "source": [
    "## CNN Classifier Testscript - Vacation Images\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f933335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import numpy as np\n",
    "import splitfolders\n",
    "import time\n",
    "from datetime import datetime \n",
    "from time import gmtime, strftime\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6dfc3",
   "metadata": {},
   "source": [
    "### Some parameters to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eda52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============#\n",
    "pixel = 128    # --> 128,64,32 are valid\n",
    "#==============#\n",
    "version = f'{pixel}x{pixel}'\n",
    "#==============#\n",
    "batch_size = 8\n",
    "#==============#\n",
    "epochs = 100\n",
    "#==============#\n",
    "lr = 0.001\n",
    "#==============#\n",
    "momentum = 0.9\n",
    "#==============#\n",
    "num_workers = 4\n",
    "#==============#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad1d19",
   "metadata": {},
   "source": [
    "### Script Runtime Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e571a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_time_():    \n",
    "    start_time = time.time()\n",
    "    return(start_time)\n",
    "\n",
    "def end_time_():\n",
    "    end_time = time.time()\n",
    "    return(end_time)\n",
    "\n",
    "def Execution_time(start_time_,end_time_):\n",
    "    return(strftime(\"%H:%M:%S\",gmtime(int('{:.0f}'.format(float(str((end_time_-start_time_))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f267bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_start = start_time_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c9510",
   "metadata": {},
   "source": [
    "### Create Directory for saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f27fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  graphs/128x128  already exists\n",
      "Directory  models/128x128  already exists\n",
      "Directory  output_txt/128x128  already exists\n"
     ]
    }
   ],
   "source": [
    "dirName_1 = f'graphs/{version}'\n",
    "dirName_2 = f'models/{version}'\n",
    "dirName_3 = f'output_txt/{version}'\n",
    "\n",
    "directories = [dirName_1, dirName_2, dirName_3]\n",
    "\n",
    "# Create target Directory if don't exist\n",
    "\n",
    "for path in directories:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print(\"Directory \" , path ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"Directory \" , path ,  \" already exists\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fcc6c5",
   "metadata": {},
   "source": [
    "### Initialize textfile for prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65e1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'output_txt/{version}/cnn_net_b{batch_size}_e{epochs}.txt', 'a')\n",
    "\n",
    "print(f'The parameters for this network are: \\n\\\n",
    "        - input image size: \\t {version}\\n\\\n",
    "        - batch_size dataloader: {batch_size}\\n\\\n",
    "        - number of epcohs: \\t {epochs}\\n\\\n",
    "        - learning rate: \\t\\t {lr}\\n\\\n",
    "        - momentum: \\t\\t\\t {momentum}\\n\\\n",
    "        - number of workers: \\t {num_workers}','\\n', file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe40bc",
   "metadata": {},
   "source": [
    "### Initialize GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70d447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7eb09f",
   "metadata": {},
   "source": [
    "### Creating Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e57b9",
   "metadata": {},
   "source": [
    "The original folder containing the 2 subclass folder has to be split into subfolders for train, validation and test.\n",
    "\n",
    "`Original:`  \n",
    "* Data\n",
    "    * Person\n",
    "    * Scene\n",
    "---\n",
    "`New:`\n",
    "* Data\n",
    "    * train (70% of total)\n",
    "        * Person\n",
    "        * Scene\n",
    "    * val (10% of total)\n",
    "        * Person\n",
    "        * Scene\n",
    "    * test (20% of total)\n",
    "        * Person\n",
    "        * Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c99b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following Code does the described above\n",
    "\n",
    "# input_folder = 'Data/'\n",
    "# output_folder = 'img_data/'\n",
    "\n",
    "# splitfolders.ratio(input_folder, output=output_folder, seed=1337, ratio=(0.7, 0.1, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bce0b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to be used for dataloaders later on\n",
    "path_to_train = './img_data/train/'\n",
    "path_to_val = './img_data/val/'\n",
    "path_to_test = './img_data/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef23ff",
   "metadata": {},
   "source": [
    "### Define data tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5be7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4939, 0.5189, 0.5326] #calculated in seperate notebook file\n",
    "std = [0.2264, 0.2398, 0.2707] #calculated in seperate notebook file\n",
    "\n",
    "\n",
    "#for training a litte augmentation (random flip, grayscale and rotation) will be performed\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize([pixel,]),\n",
    "    T.CenterCrop([pixel,]),\n",
    "    T.RandomHorizontalFlip(0.3), \n",
    "    T.RandomGrayscale(0.1),\n",
    "    T.RandomRotation(30),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(torch.tensor(mean), torch.tensor(std))\n",
    "])\n",
    "\n",
    "#no augmentation needed, therefore only reszing, to tensor and normalizing\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize([pixel,]),\n",
    "    T.CenterCrop([pixel,]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(torch.tensor(mean), torch.tensor(std))\n",
    "])\n",
    "\n",
    "#no augmentation needed, therefore only reszing, to tensor and normalizing\n",
    "test_transforms = T.Compose([\n",
    "    T.Resize([pixel,]),\n",
    "    T.CenterCrop([pixel,]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(torch.tensor(mean), torch.tensor(std))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e1122",
   "metadata": {},
   "source": [
    "### Initialize Datasets (Train, Validation, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9f28981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets, labling is done with \"ImageFolder\"-Method\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=path_to_train, transform=train_transforms)\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=path_to_val, transform=val_transforms)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=path_to_test, transform=test_transforms)\n",
    "\n",
    "classes = ('person', 'scene')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9a074",
   "metadata": {},
   "source": [
    "### Initialize DataLoaders for the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84d72e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d28b68",
   "metadata": {},
   "source": [
    "### Unnormalize pictures for visualization later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a98f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803ebf4",
   "metadata": {},
   "source": [
    "### Define Image viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b33f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    de_norm = UnNormalize(mean=(0.4939, 0.5189, 0.5326), std=(0.2264, 0.2398, 0.2707))\n",
    "    img = de_norm(img)      # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db420b",
   "metadata": {},
   "source": [
    "### Define CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4821b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, pixel):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        self.pixel = pixel\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        #_________128x128__________________________\n",
    "        if self.pixel == 128:\n",
    "            self.conv1 = nn.Conv2d(3, 6, 5) \n",
    "            self.conv2 = nn.Conv2d(6, 16, 5, padding = 1) \n",
    "            self.conv3 = nn.Conv2d(16, 16, 5)\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 13 * 13, 120) #(16 channel * 13 * 13 (image size))\n",
    "            \n",
    "        #_________64x64____________________________\n",
    "        elif self.pixel == 64:\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3, padding = 1) \n",
    "            self.conv2 = nn.Conv2d(6, 16, 3, padding = 1) \n",
    "            self.conv3 = nn.Conv2d(16, 16, 3)\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 7 * 7, 120) #(16 channel * 7 * 7 (image size))\n",
    "            \n",
    "        #_________32x32____________________________\n",
    "        elif self.pixel == 32:\n",
    "            self.conv1 = nn.Conv2d(3, 6, 3, padding = 1) \n",
    "            self.conv2 = nn.Conv2d(6, 16, 3, padding = 1) \n",
    "            self.conv3 = nn.Conv2d(16, 16, 3)\n",
    "            \n",
    "            self.fc1 = nn.Linear(16 * 3 * 3, 120) #(16 channel * 3 * 3 (image size))\n",
    "            \n",
    "        #________following_linear_layers___________\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 20)\n",
    "        self.fc4 = nn.Linear(20, 2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7adaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(pixel).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b483bcc",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431304e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde0def",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ada73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitted from below, to run more epochs if needed without setting loss lists to zero\n",
    "train_loss = []  \n",
    "validation_loss = []\n",
    "min_valid_loss = np.inf  #value to distinguish if model will be saved or not. If val loss lower, model will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/261 [00:00<?, ?it/s]c:\\users\\matze\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "  0%|                                                                                          | 0/261 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.677907 \t Validation Loss: 0.673712 \t Validation Accuracy: 59%\n",
      "Validation Loss Decreased(inf--->0.673712) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/261 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t Training Loss: 0.673788 \t Validation Loss: 0.673416 \t Validation Accuracy: 59%\n",
      "Validation Loss Decreased(0.673712--->0.673416) \t Saving The Model\n"
     ]
    }
   ],
   "source": [
    "#Starting Time \n",
    "train_start = start_time_()\n",
    "\n",
    "\n",
    "#Training Iterations\n",
    "#==========================================================================\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    \n",
    "    #model training\n",
    "    #______________________________________________________\n",
    "    running_loss = 0.0\n",
    "    cnn.train()\n",
    "    for data, labels in tqdm(train_loader, leave=False):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    \n",
    "    #model validation\n",
    "    #______________________________________________________\n",
    "    valid_loss = 0.0\n",
    "    val_correct = 0\n",
    "    cnn.eval()\n",
    "    for data, labels in tqdm(val_loader, leave=False):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = cnn(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # get val_accuracy every epoch\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    \n",
    "    #metrics\n",
    "    #______________________________________________________\n",
    "    train_loss.append(running_loss / len(train_dataset))\n",
    "    validation_loss.append(valid_loss / len(val_dataset))\n",
    "    val_accuracy = int(100 * val_correct / len(val_dataset))\n",
    "\n",
    "    \n",
    "    #console output for tracking\n",
    "    #______________________________________________________\n",
    "    console = f'Epoch {epoch+1} \\t Training Loss: {(running_loss / len(train_dataset)):.6f} \\t Validation Loss: {(valid_loss / len(val_dataset)):.6f} \\t Validation Accuracy: {val_accuracy}%'\n",
    "    print(console,file=f)\n",
    "    print(console)\n",
    "     \n",
    "    if min_valid_loss > (valid_loss / len(val_dataset)):\n",
    "        saved = f'Validation Loss Decreased({min_valid_loss:.6f}--->{(valid_loss / len(val_dataset)):.6f}) \\t Saving The Model'\n",
    "        print(saved, file=f)\n",
    "        print(saved)\n",
    "        min_valid_loss = (valid_loss / len(val_dataset))\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(cnn.state_dict(), f'models/{version}/cnn_net_b{batch_size}_e{epochs}.pth') \n",
    "        \n",
    "        \n",
    "#==========================================================================\n",
    "\n",
    "\n",
    "print('Finished Training','\\n', file=f)\n",
    "print('Finished Training')\n",
    "\n",
    "#End Time of Training\n",
    "train_end = end_time_()\n",
    "\n",
    "#Time needed for training\n",
    "print(\"Execution time of training:\", Execution_time(train_start, train_end),'\\n', file=f)\n",
    "print(\"Execution time of training:\", Execution_time(train_start, train_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78705e07",
   "metadata": {},
   "source": [
    "### Show loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss, label='Train_Loss')\n",
    "plt.plot(validation_loss, label='Validation_Loss')\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.legend(fontsize=13)\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.grid()\n",
    "plt.title('Loss Values', fontsize=18)\n",
    "plt.savefig(f'graphs/{version}/loss_b{batch_size}_e{epochs}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085c099",
   "metadata": {},
   "source": [
    "### Testing\n",
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251677af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a batch from the test set\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#generate predictions of testloader samples\n",
    "#deactivate droptout layer with .eval()\n",
    "cnn.eval()                 \n",
    "outputs = cnn(images)\n",
    "\n",
    "#the class with the highest energy is what we choose as prediction\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    if classes[labels[i]] == classes[predicted[i]]:\n",
    "        boolean = 'Right'\n",
    "    else:\n",
    "        boolean = 'Wrong'\n",
    "    print(f'Truth: {classes[labels[i]]} \\t Prediction: {classes[predicted[i]]} \\t Result: {boolean}')\n",
    "    imshow(images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd568710",
   "metadata": {},
   "source": [
    "#### Complete testset - final epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd6600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = cnn(data)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 597 test images: %d %%' % (100 * correct / total),'\\n', file=f)\n",
    "print('Accuracy of the network on the 597 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba965a",
   "metadata": {},
   "source": [
    "#### Complete testset - best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "cnn_best = CNN(pixel).to(device)\n",
    "cnn_best.load_state_dict(torch.load(f'models/{version}/cnn_net_b{batch_size}_e{epochs}.pth'))\n",
    "cnn.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = cnn_best(data)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = int(100 * correct / total)\n",
    "\n",
    "print('Accuracy of the network on the 597 test images with best iteration: %d %%' % (100 * correct / total),'\\n', file=f)\n",
    "print('Accuracy of the network on the 597 test images with best iteration: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85f21e",
   "metadata": {},
   "source": [
    "### Script Runtime Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End Time of Training\n",
    "script_end = end_time_()\n",
    "\n",
    "#Time needed for script\n",
    "print(f'Execution time of script is : {Execution_time(script_start, script_end)} hh:mm:ss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7755ff4",
   "metadata": {},
   "source": [
    "### Closing output textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
